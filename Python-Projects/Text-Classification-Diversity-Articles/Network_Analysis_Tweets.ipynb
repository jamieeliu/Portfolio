{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network-Analysis-Tweets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBD9Im7TyujL"
      },
      "source": [
        "# Import Packages & Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEr-dIfv90fP"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import csv\n",
        "import zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3YNd42k0TMO",
        "outputId": "8b0e98f3-9f6d-4070-b485-c32ef9456619"
      },
      "source": [
        "!pip install demoji\n",
        "import demoji\n",
        "demoji.download_codes()\n",
        "import nltk\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "ps = nltk.PorterStemmer()\n",
        "import re\n",
        "import shutil\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "import string\n",
        "import itertools\n",
        "punctuation = string.punctuation\n",
        "#Adding stop words\n",
        "stopwordsset = set(stopwords.words(\"english\"))\n",
        "stopwordsset.add('rt')\n",
        "stopwordsset.add(\"'s\")\n",
        "stopwordsset.add('...')\n",
        "stopwordsset.add('...')\n",
        "stopwordsset.add('\"')\n",
        "stopwordsset.add('na')\n",
        "stopwordsset.add('re')\n",
        "stopwordsset.add('u')\n",
        "stopwordsset.add('x')\n",
        "stopwordsset.add('@')\n",
        "stopwordsset.add(':')\n",
        "stopwordsset.add('#')\n",
        "stopwordsset.add(\"'\")\n",
        "stopwordsset.add(\"’\")\n",
        "stopwordsset.add(\"‘\")\n",
        "stopwordsset.add('q')\n",
        "stopwordsset.add('n')\n",
        "stopwordsset.add('1')\n",
        "stopwordsset.add('w')\n",
        "stopwordsset.add('2')\n",
        "stopwordsset.add('6')\n",
        "stopwordsset.add('m')\n",
        "stopwordsset.add('–')\n",
        "stopwordsset.add('“')\n",
        "stopwordsset.add('”')\n",
        "stopwordsset.add('5')\n",
        "stopwordsset.add('im')\n",
        "stopwordsset.add(\"‘\")\n",
        "stopwordsset.add('amp')\n",
        "stopwordsset.add('2021')\n",
        "stopwordsset.add('…')\n",
        "stopwordsset.add('210310')\n",
        "stopwordsset.add('the...')\n",
        "stopwordsset.add('the…')\n",
        "stopwordsset.add('check')\n",
        "stopwordsset.add('nothing')\n",
        "stopwordsset.add('maybe')\n",
        "stopwordsset.add('see')\n",
        "stopwordsset.add('let')\n",
        "stopwordsset.add('nayeon')\n",
        "stopwordsset.add('one')\n",
        "stopwordsset.add('first')\n",
        "stopwordsset.add('come')\n",
        "stopwordsset.add('soon')\n",
        "stopwordsset.add('time')\n",
        "stopwordsset.add('finally')\n",
        "stopwordsset.add('think')\n",
        "stopwordsset.add('please')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting demoji\n",
            "  Downloading https://files.pythonhosted.org/packages/88/6a/34379abe01c9c36fe9fddc4181dd935332e7d0159ec3fae76f712e49bcea/demoji-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.7/dist-packages (from demoji) (2.23.0)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n",
            "Installing collected packages: colorama, demoji\n",
            "Successfully installed colorama-0.4.4 demoji-0.4.0\n",
            "Downloading emoji data ...\n",
            "... OK (Got response in 0.16 seconds)\n",
            "Writing emoji data to /root/.demoji/codes.json ...\n",
            "... OK\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfgzoAeCQMmJ",
        "outputId": "3ee47d07-d164-4fa9-bbb9-f5eae0ca5cb6"
      },
      "source": [
        "#connec to Google Drive to access fils\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2iBARi-s-aX"
      },
      "source": [
        "WORKING_DIR = \"/content/drive/MyDrive/APRD6343/NetworkAnalysis\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_vJqqmv90fe"
      },
      "source": [
        "#Return the names of the zip files in the directory as a list\n",
        "tweetzipfiles = glob.glob('%s/tweets/*.zip' % WORKING_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg-bsOjS90fh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1670e5dc-5da3-4e95-dc25-173dbc49046a"
      },
      "source": [
        "tweetzipfiles"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/APRD6343/NetworkAnalysis/tweets/balenciaga_lang:en.zip',\n",
              " '/content/drive/MyDrive/APRD6343/NetworkAnalysis/tweets/louis_vuitton_lang:en.zip',\n",
              " '/content/drive/MyDrive/APRD6343/NetworkAnalysis/tweets/gucci_lang:en.zip']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27blx2mRyymk"
      },
      "source": [
        "# Identify Unique Users in the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGZpQt_A90fk"
      },
      "source": [
        "# Identify unique users in the mention network\n",
        "uniqueusers = {}\n",
        "\n",
        "for tweetzipfile in tweetzipfiles: #Each file represents a zip file containing multiple tweets\n",
        "  zf = zipfile.ZipFile(tweetzipfile) #Load zip file\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    userwhotweeted = tweetjson['user']['screen_name']  #Extract usernames from tweets\n",
        "    followercount = tweetjson['user']['followers_count']\n",
        "    if followercount > 15000: #Look at Twitter users with over 15k followers\n",
        "      if userwhotweeted in uniqueusers:\n",
        "        uniqueusers[userwhotweeted] += 1 \n",
        "      if userwhotweeted not in uniqueusers: \n",
        "        uniqueusers[userwhotweeted] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKgVLQxV90fq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36101b3d-ad1b-4601-bf8a-cf5d45cb4b7e"
      },
      "source": [
        "len(uniqueusers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "366"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQdOj_xn90fu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0acaead8-c2e4-4963-e963-356204476d3a"
      },
      "source": [
        "#Narrow down list of users to analyze\n",
        "userstoinclude = set()\n",
        "usercount = 0\n",
        "for auser in uniqueusers:\n",
        "    if uniqueusers[auser] != 0:\n",
        "        usercount += 1\n",
        "        userstoinclude.add(auser)\n",
        "        \n",
        "print(len(userstoinclude))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "366\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVcMBouCy61G"
      },
      "source": [
        "# Export Mentions Edgelist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOV39NA690fx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c88b53-fc07-4766-f2c5-c528bc613e7b"
      },
      "source": [
        "#Edgelist is a list of all connections beetween the Twitter users\n",
        "edgelist = open('%s/designer15k.edgelist.for.gephi.csv' % WORKING_DIR, 'w')\n",
        "csvwriter = csv.writer(edgelist)\n",
        "header = ['Source', 'Target']\n",
        "csvwriter.writerow(header)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUY7MwWw90fz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "272eff14-d0f8-4f07-9155-7164f76f9e41"
      },
      "source": [
        "#Open one tweet file at a time, then load the json into the dictionary tweetjson\n",
        "\n",
        "print('Writing edge list')\n",
        "count = 0\n",
        "\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  count += 1\n",
        "  if count % 1000 == 0:\n",
        "    print(count)\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()): #Enumerate each file in the zip file\n",
        "    tweetjson = json.load(zf.open(obj)) \n",
        "    userwhotweeted = tweetjson['user']['screen_name'] #Pull out Twitter usernames\n",
        "    if userwhotweeted in userstoinclude: \n",
        "#Within the tweet metadata, in the entities section, user_mentions field contains the entity that the current tweet mentions\n",
        "      users = tweetjson['entities']['user_mentions'] #Pull who the tweet mentioned \n",
        "      if len(users) > 0:\n",
        "          for auser in user\n",
        "                screenname = auser['screen_name']\n",
        "                row = [userwhotweeted, screenname] #List of users who sent out the tweet and person they mentioned in the tweet\n",
        "                csvwriter.writerow(row)\n",
        "edgelist.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing edge list\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7bNT7Yn0jPj"
      },
      "source": [
        "# Preprocess Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM6N9ztT0BcM"
      },
      "source": [
        "#Removing urls\n",
        "def removeURL(text):\n",
        "  result = re.sub(r\"http\\S+\", \"\", text)\n",
        "  return result\n",
        "\n",
        "#Extracting contextual words from a sentence\n",
        "#Tokenizing: taking out all the words in a sentence and turning it into tokens/words\n",
        "def tokenize(text):\n",
        "  #lower case\n",
        "  text = text.lower()\n",
        "  #split into individual words\n",
        "  words = word_tokenize(text)\n",
        "  return words\n",
        "\n",
        "#Stem: reduce the number of repeated words, example - peaches : peach\n",
        "def stem(tokenizedtext):\n",
        "  rootwords = []\n",
        "  for aword in tokenizedtext:\n",
        "    aword = ps.stem(aword)\n",
        "    rootwords.append(aword)\n",
        "  return rootwords\n",
        "\n",
        "#Remove uninformative words such as a, an, the\n",
        "def stopWords(tokenizedtext):\n",
        "  goodwords = []\n",
        "  for aword in tokenizedtext:\n",
        "    if aword not in stopwordsset:\n",
        "      goodwords.append(aword)\n",
        "  return goodwords\n",
        "\n",
        "#Feature reduction: getting the roots of words and graphing only the root words\n",
        "def lemmatizer(tokenizedtext):\n",
        "  lemmawords = []\n",
        "  for aword in tokenizedtext:\n",
        "    aword = wn.lemmatize(aword)\n",
        "    lemmawords.append(aword)\n",
        "  return lemmawords\n",
        "\n",
        "#Input a list of tokens and return a list of unpunctuated tokens/words\n",
        "def removePunctuation(tokenizedtext):\n",
        "  nopunctwords = []\n",
        "  for aword in tokenizedtext:\n",
        "    if aword not in punctuation:\n",
        "      nopunctwords.append(aword)\n",
        "  cleanedwords = []\n",
        "  for aword in nopunctwords:\n",
        "    aword = aword.translate(str.maketrans('', '', string.punctuation))\n",
        "    cleanedwords.append(aword)\n",
        "  return cleanedwords\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SaIBOj_0qtu"
      },
      "source": [
        "# Identify Unique Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKw6v3D20n3C",
        "outputId": "2e844d3f-1589-4b2f-b9e1-400884c4a5f4"
      },
      "source": [
        "from time import sleep \n",
        "\n",
        "uniquewords = {}\n",
        "count = 0\n",
        "\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    count += 1\n",
        "    if count % 1000 == 0:\n",
        "        print(count)\n",
        "    \n",
        "    text = tweetjson['text']\n",
        "    #Remove emojis\n",
        "    noemoji = demoji.replace(text, '')\n",
        "    #Natural language preprocessing: clean the tweet \n",
        "    nourlstext = removeURL(noemoji)\n",
        "    tokenizedtext = tokenize(nourlstext)\n",
        "    nostopwordstext = stopWords(tokenizedtext)\n",
        "    lemmatizedtext = lemmatizer(nostopwordstext)\n",
        "    nopuncttext = removePunctuation(lemmatizedtext)\n",
        "    \n",
        "    #print(tokenizedtext)\n",
        "    #print(nostopwordstext)\n",
        "    #print(lemmatizedtext)\n",
        "    #print(nopuncttext)\n",
        "    #sleep(10)\n",
        "\n",
        "    for aword in nopuncttext: #Counting number of unique words\n",
        "        if aword in uniquewords:\n",
        "            uniquewords[aword] += 1\n",
        "        if aword not in uniquewords:\n",
        "            uniquewords[aword] = 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqbcecAH0uNA"
      },
      "source": [
        "#List of unique words: key - word; value = number of times the word appears\n",
        "#Go through dictionary one at time - if word was mentioned more than n times, include it\n",
        "wordstoinclude = set()\n",
        "wordcount = 0\n",
        "for aword in uniquewords:\n",
        "    if uniquewords[aword] > 250: #Play with this number to get around 100-200 words to work with\n",
        "        wordcount += 1\n",
        "        wordstoinclude.add(aword) #Set of words used as filter - if word is in this set, include it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc0KBJ5Y0ubw",
        "outputId": "fa908dff-4f80-4b95-960d-bcccbc3efd2c"
      },
      "source": [
        "print(wordcount)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R572v04c0wvH"
      },
      "source": [
        "# Create Tweet Edgelist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIcAkz7s0vbF",
        "outputId": "a14a75e4-56d1-493a-8c2d-40f3dd787f7b"
      },
      "source": [
        "edgelist = open('%s/designer15ksemantic.edgelist.for.gephi.csv' % WORKING_DIR,'w')\n",
        "csvwriter = csv.writer(edgelist)\n",
        "\n",
        "header = ['Source', 'Target', 'Type']\n",
        "csvwriter.writerow(header)\n",
        "\n",
        "print('Writing Edge List')\n",
        "\n",
        "uniquewords = {}\n",
        "count = 0\n",
        "\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    count += 1\n",
        "    if count % 1000 == 0:\n",
        "        print(count)\n",
        "    \n",
        "    text = tweetjson['text']\n",
        "    #Natural language preprocessing: clean the tweet \n",
        "    nourlstext = removeURL(text)\n",
        "    tokenizedtext = tokenize(nourlstext)\n",
        "    nostopwordstext = stopWords(tokenizedtext)\n",
        "    lemmatizedtext = lemmatizer(nostopwordstext)\n",
        "    nopuncttext = removePunctuation(lemmatizedtext)\n",
        "\n",
        "    goodwords = [] #Only include words mentioned above certain amount of times\n",
        "    for aword in nopuncttext:\n",
        "        if aword in wordstoinclude: #Filter if word was in set of words from above\n",
        "            goodwords.append(aword.replace(',', ''))\n",
        "            \n",
        "    allcombos = itertools.combinations(goodwords, 2) #Set this to the number of words wanted in each combination\n",
        "    for acombo in allcombos:\n",
        "        row = []\n",
        "        for anode in acombo:\n",
        "            row.append(anode)\n",
        "        row.append('Undirected')\n",
        "        csvwriter.writerow(row)\n",
        "\n",
        "edgelist.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing Edge List\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}